---
title: "**기업별 반도체 특허 시각화 및 분석**"
subtitle: "**S**tatistics **N**etwork **A**nalysis"
author: "조사방법론2 <br> 2조 <br> 가천대학교 응용통계학과 <br> 김민형, 박민지, 이건영, 이승민"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    fig_caption: yes
    fig_height: 7
    fig_width: 10
    highlight: haddock
    number_sections: yes
    pandoc_args:
    - --number-sections
    - --number-offset=0
    self_contained: no
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '3'
---

<style type="text/css">
  body, td {
     font-size: 16px;
     font-family: 맑은 고딕
  }
  code.r{
    font-size: 16px;
    font-weight: bold;
    font-family: 맑은 고딕
  }
  pre {
    font-size: 14px
    font-family: 맑은 고딕
  }
  h1,h2,h3,h4,h5,h6{
    font-family: 맑은 고딕;
  }
  h1{
    font-size: 22pt;
  }
  h2{
    font-size: 20pt;
  }
  h3{
    font-size: 18pt;
  }
</style>

<style>
#TOC {
top:5%;
}
.list-group-item.active, .list-group-item.active:focus,.list-group-item.active:hover {
background-color: skyblue;
color: black;
border-color: gray;
}
</style>

<br>

> [2조 GITHUB](https://github.com/tmdals5252/Statistics-Network-Analysis/ "2조 깃허브 자료 및 코드 모음.")

<https://github.com/tmdals5252/Statistics-Network-Analysis>
---

<br>

```{r, include = FALSE}
library(multilinguer)
library(stringr) 
library(wordcloud2) 
library(KoNLP)
library(wordcloud)
library(RColorBrewer)
library(tidyverse)
library(ggfortify)
library(ggpubr)
library(ggthemes)
library(olsrr)
library(ggplot2) 
library(dplyr) 
library(tm)
library(magrittr)
library(patchwork)
library(GGally)
library(gridExtra)
library(ggbreak)
```


---

<br>

# <1> 개요 {-}

<br>

<div class="warning" style='padding:0.1em; background-color:#F0F8FF; color:#000000'>
<span>
<p style='margin-top:1em; text-align:center'>
<b></b></p>
<p style='margin-left:1em;'>

## 1. 주제 : **반도체 특허 분석 및 시각자료 제공** {-}

기업별 반도체 특허를 시각자료로 제공하고 분석한다.

<br>

## 2. 대상 {-}
- <span style="color: #6A5ACD">특허를 내는 기업</span>

- <span style="color: #6A5ACD">변리사</span>

- <span style="color: #6A5ACD">특허청 공무원</span>

<br>

## 3. 선정 기업 및 반도체 {-}
- 선정 기업 : LG, SAMSUNG, SK

- 반도체
    - **Display**
      <br>
      컴퓨터로 처리된 내용을 브라운관에 보여주는 출력장치
      
    - **Led** (Light-Emitting Diode)
      <br>
      전기 에너지를 빛으로 변환시키는 광반도체
      
    - **RAM** (Random Access Memory)
      <br>
      기억된 정보를 읽어내기도 하고 다른 정보를 기억시킬 수도 있는 메모리
       
    - **Transistor**
      <br>
      전류나 전압 흐름을 조절하여 신호를 증폭하고, 스위치 역할 등을 수행하는 반도체소자
    
<br>

## 4. 기대 효과 {-}
- 기업 Issue
    - 어느 기업이, 어느 부품에서 특허를 많이 가지고 있는지 알 수 있다.

- 반도체 Issue
    - 반도체 내에서 어떤 부품이 issue가 되는지 알 수 있다.

- 경쟁력 향상
    - 각 기업이 가지고 있는 특허 중 개선해야 할 부분을 파악하여 경쟁력 향상되는데 도움이 된다.

</p></span>
</div>

<br>

---

<br>

---

<br>

# <2> Data {-}

<br>

<div class="warning" style='padding:0.1em; background-color:#F0F8FF; color:#000000'>
<span>
<p style='margin-top:1em; text-align:center'>
<b></b></p>
<p style='margin-left:1em;'>

## 1. Data 수집 {-}
Kipris 특허청 특허정보검색서비스

<br>

[![kipris 로고](http://www.e-patentnews.com/imgdata/e-patentnews_com/202206/2022062425208055.png)](http://www.kipris.or.kr/khome/main.jsp)

> Kipris 사이트 <http://www.kipris.or.kr/khome/main.jsp>

<br>

## 2. DE {-}

  1. 분석을 진행할 변수 열 선택 (4개의 변수)
  <br>
    - 발명의 명칭, 출원인, 출원연도, 심사진행 상태
        
  2. 출원인을 기준으로 반도체 부품별 데이터 구분
  <br>
    - 3개 기업 x 4개 반도체
    - 총 12개 데이터 생성
    
<br>
    
## 3. DV {-}

  1. 워드 클라우드 시각화
  <br>
    - 발명의 명칭 변수 사용, 단어의 빈도 분석
    <br>    -> 해당 부품의 특허에서 가장 많이 언급되는 단어 파악
    
<br>
  2. 심사 진행상태 비율 시각화
    <br>
    - 심사 진행 상태 변수를 사용
      - 각 기업이 각 부품 별 가지고 있는 특허의 등록결정, 등록거절, 취하, 포기 등 비율 시각화

<br>
  3. 각 부품별 기업의 특허 개수 비율 시각화
    <br>
    - 해당 부품에서 가장 많은 특허를 가진 기업 파악

<br>   
  4. 특허 추이 시각화
    <br>
    - 연도별 심사 진행 상태의 비율 추이 시각화
    <br>
    - 1의 워드클라우드 키워드 중 상위 키워드를 선별하여 연도별 키워드 빈도수의 추이 시각화

<br>
</p></span>
</div>

<br>

---

<br>

---

<br>

# <3> 기업별 워드클라우드 {-}

<br>

## 1. 워드클라우드 코드 {-}

> 예시

### 1). 데이터 가져오기 {-}

```{r, results = "hide", message=FALSE, warning=FALSE}
##sk led

led_sk <- file("C:/Users/dnjs1/Downloads/Rr/led_sk.txt")
led_sk_r<-readLines(led_sk)
str(led_sk_r)
head(led_sk_r)
```

<br>

### 2). 명사 추출 {-}

```{r, results = "hide", message=FALSE, warning=FALSE}
exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


l_k_r<- sapply(led_sk_r, exNouns)
l_k_r[2]
```

<br>

### 3). 말뭉치 Corpus 생성 {-}

```{r, results = "hide", message=FALSE, warning=FALSE}
l_k_r_cpus<-Corpus(VectorSource(l_k_r))
l_k_r_cpus
```

<br>

### 4). 데이터 전처리 {-}

```{r, results = "hide", message=FALSE, warning=FALSE}
l_k_r_cpus_prepro<-tm_map(l_k_r_cpus,removePunctuation) #문장부호제거

l_k_r_cpus_prepro<-tm_map(l_k_r_cpus_prepro,removeNumbers) #숫자 제거

l_k_r_cpus_prepro<-tm_map(l_k_r_cpus_prepro,tolower) #영문자 소문자 변경

l_k_r_cpus_prepro<-tm_map(l_k_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(l_k_r_cpus_prepro)
```

<br>

### 5). 단어 선별 및 빈도수 {-}

```{r, results = "hide", message=FALSE, warning=FALSE}
#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
l_k_r_cpus_prepro_term<-TermDocumentMatrix(l_k_r_cpus_prepro, control = list(wordLengths=c(4,16)))

l_k_r_cpus_prepro_term

#matrix -> data.frame으로 변경
l_k_r_cpus_df<-as.data.frame(as.matrix(l_k_r_cpus_prepro_term))

dim(l_k_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult<- sort(rowSums(l_k_r_cpus_df), decreasing = T)
wordResult
```

<br>

### 6). 워드 클라우드 시각화 {-}

```{r, results = "hide", message=FALSE, warning=FALSE}
#워드 클라우드 시각화
word_names<-names(wordResult)
wordcloud(names(wordResult[1:40]),wordResult[1:40]) # 상위 30개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult[1:40]), freq= wordResult[1:40])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```

<br>

### 7). 단어 구름 시각화 {-}
```{r}
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")
```

<br>

---

<br>

---

<br>

## 2. LG {-}

<br>

>

### **Display** {-}

```{r, include = FALSE}
##LG dis
dis_LG <- file("C:/Users/dnjs1/Downloads/Rr/dis_n(LG).txt")
dis_LG_r<-readLines(dis_LG)
str(dis_LG_r)
head(dis_LG_r)
```


```{r, include = FALSE}
#명사 추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


d_L_r<- sapply(dis_LG_r, exNouns)
d_L_r[2]
```


```{r, include = FALSE}
#말뭉치 Corpus 생성

d_L_r_cpus<-Corpus(VectorSource(d_L_r))
d_L_r_cpus
```


```{r, include = FALSE}
#데이터 전처리

d_L_r_cpus_prepro<-tm_map(d_L_r_cpus,removePunctuation) #문장부호제거

d_L_r_cpus_prepro<-tm_map(d_L_r_cpus_prepro,removeNumbers) #숫자 제거

d_L_r_cpus_prepro<-tm_map(d_L_r_cpus_prepro,tolower) #영문자 소문자 변경

d_L_r_cpus_prepro<-tm_map(d_L_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(d_L_r_cpus_prepro)
```


```{r, include = FALSE}
#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
d_L_r_cpus_prepro_term<-TermDocumentMatrix(d_L_r_cpus_prepro, control = list(wordLengths=c(4,16)))

d_L_r_cpus_prepro_term

#matrix -> data.frame으로 변경
d_L_r_cpus_df<-as.data.frame(as.matrix(d_L_r_cpus_prepro_term))

dim(d_L_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult1<- sort(rowSums(d_L_r_cpus_df), decreasing = T)
wordResult1
```

```{r, include = FALSE}
#워드 클라우드 시각화
word_names<-names(wordResult1)
wordcloud(names(wordResult1[1:30]),wordResult1[1:30]) # 상위 30개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult1[1:40]), freq= wordResult1[1:40])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")

```

<br>

---

<br>

>

### **Led** {-}

```{r, include = FALSE}
led_LG <- file("C:/Users/dnjs1/Downloads/Rr/led(LG).txt")
led_LG_r<-readLines(led_LG)
str(led_LG_r)
head(led_LG_r)
```

<br>

```{r, include = FALSE}
#명사추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


l_L_r<- sapply(led_LG_r, exNouns)
l_L_r[2]

```


<br>

```{r, include = FALSE}
#말뭉치 corpus 생성

l_L_r_cpus<-Corpus(VectorSource(l_L_r))
l_L_r_cpus

```

<br>

```{r, include = FALSE}
# 데이터 전처리 및 결과 확인

l_L_r_cpus_prepro<-tm_map(l_L_r_cpus,removePunctuation) #문장부호제거

l_L_r_cpus_prepro<-tm_map(l_L_r_cpus_prepro,removeNumbers) #숫자 제거

l_L_r_cpus_prepro<-tm_map(l_L_r_cpus_prepro,tolower) #영문자 소문자 변경

l_L_r_cpus_prepro<-tm_map(l_L_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(l_L_r_cpus_prepro)

```

<br>

```{r, include = FALSE}
#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
l_L_r_cpus_prepro_term<-TermDocumentMatrix(l_L_r_cpus_prepro, control = list(wordLengths=c(4,16)))

l_L_r_cpus_prepro_term

#matrix -> data.frame으로 변경
l_L_r_cpus_df<-as.data.frame(as.matrix(l_L_r_cpus_prepro_term))

dim(l_L_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult2<- sort(rowSums(l_L_r_cpus_df), decreasing = T)
wordResult2
```

<br>

```{r, include = FALSE}
#워드 클라우드 시각화
word_names<-names(wordResult2)
wordcloud(names(wordResult2[1:40]),wordResult2[1:40]) # 상위 30개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult2[1:40]), freq= wordResult2[1:40])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")

```

<br>

---

<br>

>

### **Ram** {-}

```{r, include = FALSE}
RAM_LG <- file("C:/Users/dnjs1/Downloads/Rr/RAM(LG).txt")
RAM_LG_r<-readLines(RAM_LG)
str(RAM_LG_r)
head(RAM_LG_r)

```

```{r, include = FALSE}
#명사 추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


r_L_r<- sapply(RAM_LG_r, exNouns)
r_L_r[2]


#말뭉치 Corpus 생성

r_L_r_cpus<-Corpus(VectorSource(r_L_r))
r_L_r_cpus


#데이터 전처리

r_L_r_cpus_prepro<-tm_map(r_L_r_cpus,removePunctuation) #문장부호제거

r_L_r_cpus_prepro<-tm_map(r_L_r_cpus_prepro,removeNumbers) #숫자 제거

r_L_r_cpus_prepro<-tm_map(r_L_r_cpus_prepro,tolower) #영문자 소문자 변경

r_L_r_cpus_prepro<-tm_map(r_L_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(r_L_r_cpus_prepro)


#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
r_L_r_cpus_prepro_term<-TermDocumentMatrix(r_L_r_cpus_prepro, control = list(wordLengths=c(4,16)))

r_L_r_cpus_prepro_term

#matrix -> data.frame으로 변경
r_L_r_cpus_df<-as.data.frame(as.matrix(r_L_r_cpus_prepro_term))

dim(r_L_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult3<- sort(rowSums(r_L_r_cpus_df), decreasing = T)
wordResult3


#워드 클라우드 시각화
word_names<-names(wordResult3)
wordcloud(names(wordResult3[1:40]),wordResult3[1:40]) # 상위 40개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult3[1:30]), freq= wordResult3[1:30])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")

```


<br>

---

<br>

>

### **Transistor** {-}

```{r, include = FALSE}
Tran_LG <- file("C:/Users/dnjs1/Downloads/Rr/Transistor(LG).txt")
Tran_LG_r<-readLines(Tran_LG)
str(Tran_LG_r)
head(Tran_LG_r)

```

```{r, include = FALSE}
#명사 추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


t_L_r<- sapply(Tran_LG_r, exNouns)
t_L_r[2]


#말뭉치 Corpus 생성

t_L_r_cpus<-Corpus(VectorSource(t_L_r))
t_L_r_cpus


#데이터 전처리

t_L_r_cpus_prepro<-tm_map(t_L_r_cpus,removePunctuation) #문장부호제거

t_L_r_cpus_prepro<-tm_map(t_L_r_cpus_prepro,removeNumbers) #숫자 제거

t_L_r_cpus_prepro<-tm_map(t_L_r_cpus_prepro,tolower) #영문자 소문자 변경

t_L_r_cpus_prepro<-tm_map(t_L_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(t_L_r_cpus_prepro)


#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
t_L_r_cpus_prepro_term<-TermDocumentMatrix(t_L_r_cpus_prepro, control = list(wordLengths=c(4,16)))

t_L_r_cpus_prepro_term

#matrix -> data.frame으로 변경
t_L_r_cpus_df<-as.data.frame(as.matrix(t_L_r_cpus_prepro_term))

dim(t_L_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult4<- sort(rowSums(t_L_r_cpus_df), decreasing = T)
wordResult4


#워드 클라우드 시각화
word_names<-names(wordResult4)
wordcloud(names(wordResult4[1:40]),wordResult4[1:40]) # 상위 40개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult4[1:40]), freq= wordResult4[1:40])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")

```

<br>

---

<br>

---

<br>

## 3. Sansung {-}

<br>

>

### **Display** {-}

```{r, include = FALSE}
##Samsung dis

dis_s <- file("C:/Users/dnjs1/Downloads/Rr/dis_sam.txt")
dis_s_r<-readLines(dis_s)
str(dis_s_r)
head(dis_s_r)

#명사 추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


d_s_r<- sapply(dis_s_r, exNouns)
d_s_r[2]


#말뭉치 Corpus 생성

d_s_r_cpus<-Corpus(VectorSource(d_s_r))
d_s_r_cpus


#데이터 전처리

d_s_r_cpus_prepro<-tm_map(d_s_r_cpus,removePunctuation) #문장부호제거

d_s_r_cpus_prepro<-tm_map(d_s_r_cpus_prepro,removeNumbers) #숫자 제거

d_s_r_cpus_prepro<-tm_map(d_s_r_cpus_prepro,tolower) #영문자 소문자 변경

d_s_r_cpus_prepro<-tm_map(d_s_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(d_s_r_cpus_prepro)


#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
d_s_r_cpus_prepro_term<-TermDocumentMatrix(d_s_r_cpus_prepro, control = list(wordLengths=c(4,16)))

d_s_r_cpus_prepro_term

#matrix -> data.frame으로 변경
d_s_r_cpus_df<-as.data.frame(as.matrix(d_s_r_cpus_prepro_term))

dim(d_s_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult5<- sort(rowSums(d_s_r_cpus_df), decreasing = T)
wordResult5


#워드 클라우드 시각화
word_names<-names(wordResult5)
wordcloud(names(wordResult5[1:30]),wordResult5[1:30]) # 상위 30개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult5[1:40]), freq= wordResult5[1:40])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")


```

<br>

---

<br>

>

### **LED** {-}

```{r, include = FALSE}
##samsung led
l_sam <- file("C:/Users/dnjs1/Downloads/Rr/led_sam.txt")
l_sam_r<-readLines(l_sam)
str(l_sam_r)
head(l_sam_r)

#명사 추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


l_s_r<- sapply(l_sam_r, exNouns)
l_s_r[2]


#말뭉치 Corpus 생성

l_s_r_cpus<-Corpus(VectorSource(l_s_r))
l_s_r_cpus


#데이터 전처리

l_s_r_cpus_prepro<-tm_map(l_s_r_cpus,removePunctuation) #문장부호제거

l_s_r_cpus_prepro<-tm_map(l_s_r_cpus_prepro,removeNumbers) #숫자 제거

l_s_r_cpus_prepro<-tm_map(l_s_r_cpus_prepro,tolower) #영문자 소문자 변경

l_s_r_cpus_prepro<-tm_map(l_s_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(l_s_r_cpus_prepro)


#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
l_s_r_cpus_prepro_term<-TermDocumentMatrix(l_s_r_cpus_prepro, control = list(wordLengths=c(4,16)))

l_s_r_cpus_prepro_term

#matrix -> data.frame으로 변경
l_s_r_cpus_df<-as.data.frame(as.matrix(l_s_r_cpus_prepro_term))

dim(l_s_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult6<- sort(rowSums(l_s_r_cpus_df), decreasing = T)
wordResult6


#워드 클라우드 시각화
word_names<-names(wordResult6)
wordcloud(names(wordResult6[1:40]),wordResult6[1:40]) # 상위 30개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult6[1:40]), freq= wordResult6[1:40])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))

```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")

```

<br>

---

<br>

>

### **RAM** {-}

```{r, include = FALSE}
##samsung RAM

r_sam <- file("C:/Users/dnjs1/Downloads/Rr/ram_sam.txt")
r_sam_r<-readLines(r_sam)
str(r_sam_r)
head(r_sam_r)

#명사 추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


r_s_r<- sapply(r_sam_r, exNouns)
r_s_r[2]


#말뭉치 Corpus 생성

r_s_r_cpus<-Corpus(VectorSource(r_s_r))
r_s_r_cpus


#데이터 전처리

r_s_r_cpus_prepro<-tm_map(r_s_r_cpus,removePunctuation) #문장부호제거

r_s_r_cpus_prepro<-tm_map(r_s_r_cpus_prepro,removeNumbers) #숫자 제거

r_s_r_cpus_prepro<-tm_map(r_s_r_cpus_prepro,tolower) #영문자 소문자 변경

r_s_r_cpus_prepro<-tm_map(r_s_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(r_s_r_cpus_prepro)


#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
r_s_r_cpus_prepro_term<-TermDocumentMatrix(r_s_r_cpus_prepro, control = list(wordLengths=c(4,16)))

r_s_r_cpus_prepro_term

#matrix -> data.frame으로 변경
r_s_r_cpus_df<-as.data.frame(as.matrix(r_s_r_cpus_prepro_term))

dim(r_s_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult7<- sort(rowSums(r_s_r_cpus_df), decreasing = T)
wordResult7


#워드 클라우드 시각화
word_names<-names(wordResult7)
wordcloud(names(wordResult7[1:40]),wordResult7[1:40]) # 상위 40개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult7[1:30]), freq= wordResult7[1:30])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")


```

<br>

---

<br>

>

### **Transistor** {-}

```{r, include = FALSE}
##samsung transistor


tran_sam <- file("C:/Users/dnjs1/Downloads/Rr/tran_sam.txt")
tran_sam_r<-readLines(tran_sam)
str(tran_sam_r)
head(tran_sam_r)

#명사 추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


t_s_r<- sapply(tran_sam_r, exNouns)
t_s_r[2]


#말뭉치 Corpus 생성

t_s_r_cpus<-Corpus(VectorSource(t_s_r))
t_s_r_cpus


#데이터 전처리

t_s_r_cpus_prepro<-tm_map(t_s_r_cpus,removePunctuation) #문장부호제거

t_s_r_cpus_prepro<-tm_map(t_s_r_cpus_prepro,removeNumbers) #숫자 제거

t_s_r_cpus_prepro<-tm_map(t_s_r_cpus_prepro,tolower) #영문자 소문자 변경

t_s_r_cpus_prepro<-tm_map(t_s_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(t_s_r_cpus_prepro)


#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
t_s_r_cpus_prepro_term<-TermDocumentMatrix(t_s_r_cpus_prepro, control = list(wordLengths=c(4,16)))

t_s_r_cpus_prepro_term

#matrix -> data.frame으로 변경
t_s_r_cpus_df<-as.data.frame(as.matrix(t_s_r_cpus_prepro_term))

dim(t_s_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult8<- sort(rowSums(t_s_r_cpus_df), decreasing = T)
wordResult8


#워드 클라우드 시각화
word_names<-names(wordResult8)
wordcloud(names(wordResult8[1:40]),wordResult8[1:40]) # 상위 40개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult8[1:40]), freq= wordResult8[1:40])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")

```

<br>

---

<br>

---

<br>

## 4. SK {-}

<br>

>

### **Display** {-}

```{r, include = FALSE}
##sk dis

dis_sk <- file("C:/Users/dnjs1/Downloads/Rr/dis_sk.txt")
dis_sk_r<-readLines(dis_sk)
str(dis_sk_r)
head(dis_sk_r)

#명사 추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


d_k_r<- sapply(dis_sk_r, exNouns)
d_k_r[2]


#말뭉치 Corpus 생성

d_k_r_cpus<-Corpus(VectorSource(d_k_r))
d_k_r_cpus


#데이터 전처리

d_k_r_cpus_prepro<-tm_map(d_k_r_cpus,removePunctuation) #문장부호제거

d_k_r_cpus_prepro<-tm_map(d_k_r_cpus_prepro,removeNumbers) #숫자 제거

d_k_r_cpus_prepro<-tm_map(d_k_r_cpus_prepro,tolower) #영문자 소문자 변경

d_k_r_cpus_prepro<-tm_map(d_k_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(d_k_r_cpus_prepro)


#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
d_k_r_cpus_prepro_term<-TermDocumentMatrix(d_k_r_cpus_prepro, control = list(wordLengths=c(4,16)))

d_k_r_cpus_prepro_term

#matrix -> data.frame으로 변경
d_k_r_cpus_df<-as.data.frame(as.matrix(d_k_r_cpus_prepro_term))

dim(d_k_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult9<- sort(rowSums(d_k_r_cpus_df), decreasing = T)
wordResult9


#워드 클라우드 시각화
word_names<-names(wordResult9)
wordcloud(names(wordResult9[1:30]),wordResult9[1:30]) # 상위 30개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult9[1:40]), freq= wordResult9[1:40])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")
```

<br>

---

<br>

>

### **LED** {-}

```{r, include = FALSE}
##sk led

led_sk <- file("C:/Users/dnjs1/Downloads/Rr/led_sk.txt")
led_sk_r<-readLines(led_sk)
str(led_sk_r)
head(led_sk_r)

#명사 추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


l_k_r<- sapply(led_sk_r, exNouns)
l_k_r[2]


#말뭉치 Corpus 생성

l_k_r_cpus<-Corpus(VectorSource(l_k_r))
l_k_r_cpus


#데이터 전처리

l_k_r_cpus_prepro<-tm_map(l_k_r_cpus,removePunctuation) #문장부호제거

l_k_r_cpus_prepro<-tm_map(l_k_r_cpus_prepro,removeNumbers) #숫자 제거

l_k_r_cpus_prepro<-tm_map(l_k_r_cpus_prepro,tolower) #영문자 소문자 변경

l_k_r_cpus_prepro<-tm_map(l_k_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(l_k_r_cpus_prepro)


#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
l_k_r_cpus_prepro_term<-TermDocumentMatrix(l_k_r_cpus_prepro, control = list(wordLengths=c(4,16)))

l_k_r_cpus_prepro_term

#matrix -> data.frame으로 변경
l_k_r_cpus_df<-as.data.frame(as.matrix(l_k_r_cpus_prepro_term))

dim(l_k_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult10<- sort(rowSums(l_k_r_cpus_df), decreasing = T)
wordResult10


#워드 클라우드 시각화
word_names<-names(wordResult10)
wordcloud(names(wordResult[1:40]),wordResult10[1:40]) # 상위 30개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult10[1:40]), freq= wordResult10[1:40])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")
```

<br>

---

<br>

>

### **RAM** {-}

```{r, include = FALSE}
##sk RAM

ram_sk <- file("C:/Users/dnjs1/Downloads/Rr/ram_sk.txt")
ram_sk_r<-readLines(ram_sk)
str(ram_sk_r)
head(ram_sk_r)

#명사 추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


r_k_r<- sapply(ram_sk_r, exNouns)
r_k_r[2]


#말뭉치 Corpus 생성

r_k_r_cpus<-Corpus(VectorSource(r_k_r))
r_k_r_cpus


#데이터 전처리

r_k_r_cpus_prepro<-tm_map(r_k_r_cpus,removePunctuation) #문장부호제거

r_k_r_cpus_prepro<-tm_map(r_k_r_cpus_prepro,removeNumbers) #숫자 제거

r_k_r_cpus_prepro<-tm_map(r_k_r_cpus_prepro,tolower) #영문자 소문자 변경

r_k_r_cpus_prepro<-tm_map(r_k_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(r_k_r_cpus_prepro)


#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
r_k_r_cpus_prepro_term<-TermDocumentMatrix(r_k_r_cpus_prepro, control = list(wordLengths=c(4,16)))

r_k_r_cpus_prepro_term

#matrix -> data.frame으로 변경
r_k_r_cpus_df<-as.data.frame(as.matrix(r_k_r_cpus_prepro_term))

dim(r_k_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult11<- sort(rowSums(r_k_r_cpus_df), decreasing = T)
wordResult11


#워드 클라우드 시각화
word_names<-names(wordResult11)
wordcloud(names(wordResult11[1:40]),wordResult11[1:40]) # 상위 40개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult11[1:30]), freq= wordResult11[1:30])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")
```

<br>

---

<br>

>

### **Transistor** {-}

```{r, include = FALSE}
##sk transistor


Tran_sk <- file("C:/Users/dnjs1/Downloads/Rr/tran_sk.txt")
Tran_sk_r<-readLines(Tran_sk)
str(Tran_sk_r)
head(Tran_sk_r)

#명사 추출

exNouns<-function(x){
  paste(extractNoun(as.character(x)), collpase= " ")
}


t_k_r<- sapply(Tran_sk_r, exNouns)
t_k_r[2]


#말뭉치 Corpus 생성

t_k_r_cpus<-Corpus(VectorSource(t_k_r))
t_k_r_cpus


#데이터 전처리

t_k_r_cpus_prepro<-tm_map(t_k_r_cpus,removePunctuation) #문장부호제거

t_k_r_cpus_prepro<-tm_map(t_k_r_cpus_prepro,removeNumbers) #숫자 제거

t_k_r_cpus_prepro<-tm_map(t_k_r_cpus_prepro,tolower) #영문자 소문자 변경

t_k_r_cpus_prepro<-tm_map(t_k_r_cpus_prepro,removeWords, stopwords('english')) #불용어(for, very, and, of, are) 제거


#전처리 결과 확인
inspect(t_k_r_cpus_prepro)


#단어 선별(1음절 = 2byte, 2음절 ~ 8음절)
t_k_r_cpus_prepro_term<-TermDocumentMatrix(t_k_r_cpus_prepro, control = list(wordLengths=c(4,16)))

t_k_r_cpus_prepro_term

#matrix -> data.frame으로 변경
t_k_r_cpus_df<-as.data.frame(as.matrix(t_k_r_cpus_prepro_term))

dim(t_k_r_cpus_df)

#단어 출현 빈도수 구하기

wordResult12<- sort(rowSums(t_k_r_cpus_df), decreasing = T)
wordResult12


#워드 클라우드 시각화
word_names<-names(wordResult12)
wordcloud(names(wordResult12[1:40]),wordResult12[1:40]) # 상위 40개만 뽑아서 추출

#단어 이름과 빈도수로 df 생성
word_df<-data.frame(word = names(wordResult12[1:40]), freq= wordResult12[1:40])

#단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") #12가지 색상
windowsFonts(malgun=windowsFont("맑은 고딕"))
```


```{r, echo=F}
#단어 구름 시각화
x11() #별도의 창을 띄우는 함수
wordcloud(word_df$word, word_df$freq, scale= c(5,1), min.freq=3, random.order=F, rot.per =.1, colors = pal, family= "malgun")


```

<br>

---

<br>

---

<br>

# <4>. 기업별 키워드 상위 10개 {-}

<br>

>

## 1. **LG** 상위 키워드 {-}

```{r, include = FALSE}
word_df1<-data.frame(word = names(wordResult1[1:10]), freq= wordResult1[1:10]) #lg_dis
word_df2<-data.frame(word = names(wordResult2[1:10]), freq= wordResult2[1:10]) #lg_led
word_df3<-data.frame(word = names(wordResult3[1:10]), freq= wordResult3[1:10]) #lg_ram
word_df4<-data.frame(word = names(wordResult4[1:10]), freq= wordResult4[1:10]) #lg_trans
```

<br>

### **Display** {-}
```{r, echo=F}
lg_d <- ggplot(word_df1, aes(word, freq, fill = word)) + geom_bar(stat='identity')
lg_d
```

<br>
 
### **LED** {-}
```{r, echo=F}
lg_l <- ggplot(word_df2, aes(word, freq, fill = word)) + geom_bar(stat='identity')
lg_l
```

<br>

### **RAM** {-}
```{r, echo=F}
lg_r <- ggplot(word_df3, aes(word, freq, fill = word)) + geom_bar(stat='identity')
lg_r
```

<br>

### **Transistor** {-}
```{r, echo=F}
lg_t <- ggplot(word_df4, aes(word, freq, fill = word)) + geom_bar(stat='identity')
lg_t

```

<br>

---

<br>

### LG 전체 {-}

```{r, echo=F}
lg_d <- ggplot(word_df1, aes(word, freq, fill = word)) + geom_bar(stat='identity')
lg_l <- ggplot(word_df2, aes(word, freq, fill = word)) + geom_bar(stat='identity')
lg_r <- ggplot(word_df3, aes(word, freq, fill = word)) + geom_bar(stat='identity')
lg_t <- ggplot(word_df4, aes(word, freq, fill = word)) + geom_bar(stat='identity')

grid.arrange(lg_d, lg_l, lg_r, lg_t, nrow=2, ncol=2)
```

<br>

---

<br>

---

<br>

>

## 2. **Samsung** 상위 키워드 {-}

```{r, include = FALSE}
word_df5<-data.frame(word = names(wordResult5[1:10]), freq= wordResult5[1:10]) #samsung_dis
word_df6<-data.frame(word = names(wordResult6[1:10]), freq= wordResult6[1:10]) #samsung_led
word_df7<-data.frame(word = names(wordResult7[1:10]), freq= wordResult7[1:10]) #samsung_ram
word_df8<-data.frame(word = names(wordResult8[1:10]), freq= wordResult8[1:10]) #samsung_trans
```


<br>

### **Display** {-}
```{r, echo=F}
samsung_d <- ggplot(word_df5, aes(word, freq, fill = word)) + geom_bar(stat='identity')
samsung_d
```

<br>
 
### **LED** {-}
```{r, echo=F}
samsung_l <- ggplot(word_df6, aes(word, freq, fill = word)) + geom_bar(stat='identity')
samsung_l
```

<br>

### **RAM** {-}
```{r, echo=F}
samsung_r <- ggplot(word_df7, aes(word, freq, fill = word)) + geom_bar(stat='identity')
samsung_r
```

<br>

### **Transistor** {-}
```{r, echo=F}
samsung_t <- ggplot(word_df8, aes(word, freq, fill = word)) + geom_bar(stat='identity')
samsung_t

```

<br>

---

<br>

### Samsung 전체 {-}

```{r, echo=F}
samsung_d <- ggplot(word_df5, aes(word, freq, fill = word)) + geom_bar(stat='identity')
samsung_l <- ggplot(word_df6, aes(word, freq, fill = word)) + geom_bar(stat='identity')
samsung_r <- ggplot(word_df7, aes(word, freq, fill = word)) + geom_bar(stat='identity')
samsung_t <- ggplot(word_df8, aes(word, freq, fill = word)) + geom_bar(stat='identity')

grid.arrange(samsung_d, samsung_l, samsung_r, samsung_t, nrow=2, ncol=2)
```

<br>

---

<br>

---

<br>

>

## 3. **sk** 상위 키워드 {-}

```{r, include = FALSE}
word_df9<-data.frame(word = names(wordResult9[1:10]), freq= wordResult9[1:10]) #sk_dis
word_df10<-data.frame(word = names(wordResult10[1:10]), freq= wordResult10[1:10]) #sk_led
word_df11<-data.frame(word = names(wordResult11[1:10]), freq= wordResult11[1:10]) #sk_ram
word_df12<-data.frame(word = names(wordResult12[1:10]), freq= wordResult12[1:10]) #sk_trans
```


<br>

### **Display** {-}
```{r, echo=F}
sk_d <- ggplot(word_df9, aes(word, freq, fill = word)) + geom_bar(stat='identity')
sk_d
```

<br>
 
### **LED** {-}
```{r, echo=F}
sk_l <- ggplot(word_df10, aes(word, freq, fill = word)) + geom_bar(stat='identity')
sk_l
```

<br>

### **RAM** {-}
```{r, echo=F}
sk_r <- ggplot(word_df11, aes(word, freq, fill = word)) + geom_bar(stat='identity')
sk_r
```

<br>

### **Transistor** {-}
```{r, echo=F}
sk_t <- ggplot(word_df12, aes(word, freq, fill = word)) + geom_bar(stat='identity')
sk_t

```

<br>

---

<br>

### SK 전체 {-}

```{r, echo=F}
sk_d <- ggplot(word_df9, aes(word, freq, fill = word)) + geom_bar(stat='identity')
sk_l <- ggplot(word_df10, aes(word, freq, fill = word)) + geom_bar(stat='identity')
sk_r <- ggplot(word_df11, aes(word, freq, fill = word)) + geom_bar(stat='identity')
sk_t <- ggplot(word_df12, aes(word, freq, fill = word)) + geom_bar(stat='identity')

grid.arrange(sk_d, sk_l, sk_r, sk_t, nrow=2, ncol=2)
```

<br>

---

<br>

---

<br>


ⓒ Statistical Methods, Gachon University

--------------------------------------------------------------------------------------------------
